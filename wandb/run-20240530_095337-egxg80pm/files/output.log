LOG INTO HUGGING FACE
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /home/dan/.cache/huggingface/token
Login successful
load model
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.34it/s]
tokenizer
load data
Data loaded
Traceback (most recent call last):
  File "/home/dan/mini_temporal/training/fine_tune_gemma_it.py", line 284, in <module>
    main()
  File "/home/dan/mini_temporal/training/fine_tune_gemma_it.py", line 199, in main
    dev['rel_prompt'] = format_instruction(dev, model_path = args.base_model, context = 'relevant_context')
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dan/mini_temporal/training/fine_tune_gemma_it.py", line 87, in format_instruction
    encodeds = fi_tokenizer.apply_chat_template(message, return_tensors="pt")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 1821, in apply_chat_template
    out = self(
          ^^^^^
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2883, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2989, in _call_one
    return self.encode_plus(
           ^^^^^^^^^^^^^^^^^
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 3062, in encode_plus
    return self._encode_plus(
           ^^^^^^^^^^^^^^^^^^
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py", line 614, in _encode_plus
    self._eventual_warn_about_too_long_sequence(batched_output["input_ids"], max_length, verbose)
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 3908, in _eventual_warn_about_too_long_sequence
    def _eventual_warn_about_too_long_sequence(self, ids: List[int], max_length: Optional[int], verbose: bool):
KeyboardInterrupt