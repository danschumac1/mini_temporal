LOG INTO HUGGING FACE
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /home/dan/.cache/huggingface/token
Login successful
`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.
Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use
`config.hidden_activation` if you want to override this behaviour.
See https://github.com/huggingface/transformers/pull/29402 for more details.
Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.26s/it]
lora
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
Map: 100%|██████████| 350/350 [00:00<00:00, 9518.11 examples/s]
Map: 100%|██████████| 75/75 [00:00<00:00, 6185.44 examples/s]
/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'loss': 2.8521, 'grad_norm': 2.934016704559326, 'learning_rate': 1.6750418760469013e-05, 'epoch': 0.73}
  0%|          | 0/6 [00:00<?, ?it/s]/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.
  warnings.warn(
 17%|█▋        | 1/6 [03:16<16:20, 196.07s/it]/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'loss': 2.8551, 'grad_norm': 2.9205386638641357, 'learning_rate': 1.3400335008375211e-05, 'epoch': 1.45}
 33%|███▎      | 2/6 [07:13<14:40, 220.18s/it]/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'loss': 2.8192, 'grad_norm': 2.9255001544952393, 'learning_rate': 1.005025125628141e-05, 'epoch': 2.18}
 50%|█████     | 3/6 [10:52<10:59, 219.73s/it]

 67%|██████▋   | 4/6 [14:24<07:13, 216.90s/it]/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'loss': 2.788, 'grad_norm': 3.024502754211426, 'learning_rate': 3.350083752093803e-06, 'epoch': 3.64}
 83%|████████▎ | 5/6 [17:58<03:35, 215.71s/it]/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'loss': 2.7826, 'grad_norm': 3.047814130783081, 'learning_rate': 0.0, 'epoch': 4.36}
100%|██████████| 6/6 [21:45<00:00, 219.71s/it]/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'train_runtime': 1308.3831, 'train_samples_per_second': 1.605, 'train_steps_per_second': 0.005, 'train_loss': 2.8191246589024863, 'epoch': 4.36}
Model and tokenizer saved at ./training/models/gemma-2b/random_context
100%|██████████| 6/6 [21:47<00:00, 217.85s/it]