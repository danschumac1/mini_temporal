LOG INTO HUGGING FACE
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /home/dan/.cache/huggingface/token
Login successful
`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.
Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use
`config.hidden_activation` if you want to override this behaviour.
See https://github.com/huggingface/transformers/pull/29402 for more details.
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.31it/s]
Traceback (most recent call last):
  File "/home/dan/mini_temporal/training/fine_tune_gemma_fatemeh.py", line 255, in <module>
    main()
  File "/home/dan/mini_temporal/training/fine_tune_gemma_fatemeh.py", line 166, in main
    train['no_prompt'] = generate_no_context_prompt(train, no_context_prefix_text)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: generate_no_context_prompt() takes 1 positional argument but 2 were given