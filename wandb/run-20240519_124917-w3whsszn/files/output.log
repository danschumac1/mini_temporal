LOG INTO HUGGING FACE
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /home/dan/.cache/huggingface/token
Login successful
`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.
Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use
`config.hidden_activation` if you want to override this behaviour.
See https://github.com/huggingface/transformers/pull/29402 for more details.
Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.26s/it]
lora
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
Map: 100%|██████████| 350/350 [00:00<00:00, 22871.13 examples/s]
Map: 100%|██████████| 75/75 [00:00<00:00, 11973.69 examples/s]
  0%|          | 0/6 [00:00<?, ?it/s]/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.
  warnings.warn(
{'loss': 6.1559, 'grad_norm': 10.44828987121582, 'learning_rate': 1.6750418760469013e-05, 'epoch': 0.73}
 17%|█▋        | 1/6 [00:41<03:27, 41.57s/it]
  0%|          | 0/3 [00:00<?, ?it/s]

{'eval_loss': 6.169943809509277, 'eval_runtime': 4.2024, 'eval_samples_per_second': 17.847, 'eval_steps_per_second': 0.714, 'epoch': 0.73}

 33%|███▎      | 2/6 [01:29<03:01, 45.25s/it]
  0%|          | 0/3 [00:00<?, ?it/s]

{'eval_loss': 6.090743541717529, 'eval_runtime': 4.2341, 'eval_samples_per_second': 17.713, 'eval_steps_per_second': 0.709, 'epoch': 1.45}

 50%|█████     | 3/6 [02:15<02:17, 45.83s/it]

100%|██████████| 3/3 [00:02<00:00,  1.01s/it]

{'eval_loss': 6.014281749725342, 'eval_runtime': 4.2684, 'eval_samples_per_second': 17.571, 'eval_steps_per_second': 0.703, 'epoch': 2.18}
 67%|██████▋   | 4/6 [03:02<01:32, 46.15s/it]

 67%|██████▋   | 2/3 [00:01<00:00,  1.10it/s]

{'eval_loss': 5.966705322265625, 'eval_runtime': 4.2808, 'eval_samples_per_second': 17.52, 'eval_steps_per_second': 0.701, 'epoch': 2.91}
 83%|████████▎ | 5/6 [03:49<00:46, 46.29s/it]

 67%|██████▋   | 2/3 [00:01<00:00,  1.10it/s]


100%|██████████| 6/6 [04:34<00:00, 45.85s/it]
{'loss': 5.8872, 'grad_norm': 10.18928050994873, 'learning_rate': 0.0, 'epoch': 4.36}

100%|██████████| 3/3 [00:02<00:00,  1.01s/it]
{'eval_loss': 5.919917583465576, 'eval_runtime': 4.2815, 'eval_samples_per_second': 17.517, 'eval_steps_per_second': 0.701, 'epoch': 4.36}
{'train_runtime': 280.6741, 'train_samples_per_second': 7.482, 'train_steps_per_second': 0.021, 'train_loss': 6.025126377741496, 'epoch': 4.36}

Model and tokenizer saved at ./training/models/gemma-2b/no_context