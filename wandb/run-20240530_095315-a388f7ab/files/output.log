LOG INTO HUGGING FACE
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /home/dan/.cache/huggingface/token
Login successful
load model
Loading checkpoint shards:   0%|          | 0/2 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/home/dan/mini_temporal/training/fine_tune_gemma_it.py", line 284, in <module>
    main()
  File "/home/dan/mini_temporal/training/fine_tune_gemma_it.py", line 174, in main
    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={"":0}, use_cache=False)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 3754, in from_pretrained
    ) = cls._load_pretrained_model(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4214, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 889, in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_4bit.py", line 216, in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/bitsandbytes/nn/modules.py", line 324, in to
    return self._quantize(device)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/bitsandbytes/nn/modules.py", line 288, in _quantize
    w = self.data.contiguous().cuda(device)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 47.54 GiB of which 18.88 MiB is free. Process 4175187 has 5.94 GiB memory in use. Process 4175172 has 2.33 GiB memory in use. Process 4175190 has 5.94 GiB memory in use. Process 4175178 has 2.33 GiB memory in use. Process 4175169 has 2.33 GiB memory in use. Process 4175175 has 2.33 GiB memory in use. Process 4175181 has 5.94 GiB memory in use. Process 4175184 has 5.94 GiB memory in use. Process 4176489 has 1.61 GiB memory in use. Process 4176498 has 1.99 GiB memory in use. Process 4176495 has 1.45 GiB memory in use. Including non-PyTorch memory, this process has 1.84 GiB memory in use. Process 4176501 has 2.05 GiB memory in use. Process 4176507 has 1.89 GiB memory in use. Process 4176504 has 2.05 GiB memory in use. Process 4176492 has 1.51 GiB memory in use. Of the allocated memory 1.54 GiB is allocated by PyTorch, and 11.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)