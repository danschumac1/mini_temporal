LOG INTO HUGGING FACE
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /home/dan/.cache/huggingface/token
Login successful
load model
`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.
Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use
`config.hidden_activation` if you want to override this behaviour.
See https://github.com/huggingface/transformers/pull/29402 for more details.
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]
tokenizer
load data
Data loaded
In June 2004,who made its first increase in interest rates in nearly four years? The answer is: The Federal Reserve
What was the name of the lead singer of Queen, who died of AIDS in 1991? The answer is: Freddie Mercury
Russia attacked the capital city of what region with missiles and tanks between late 1999 and early 2000? The answer is: Chechnya
In July 2003 which Labour M.P. was heavily reported for saying that Tony Blair should quit? The answer is: Clare Short
0    None
1    None
2    None
3    None
4    None
Name: no_prompt, dtype: object
Who was named president of Disney-ABC television group in 2004? The answer is: Anne Sweeney
Which pope dies at his home in the Vatican, following complications from the flu in April 2005? The answer is: Pope John Paul II
Who announces he will retire from Formula One racing in September 2006? The answer is: Michael Schumacher
Which agreement came into effect between Canada, the USA and Mexico in 1994? The answer is: North American Free Trade Agreement__or__NAFTA
None
<class 'NoneType'>
Map:   0%|          | 0/350 [00:00<?, ? examples/s]
Traceback (most recent call last):
  File "/home/dan/mini_temporal/training/fine_tune_gemma_halfway.py", line 270, in <module>
    main()
  File "/home/dan/mini_temporal/training/fine_tune_gemma_halfway.py", line 193, in main
    train_data = other_preprocessing(train, tokenizer, args.model_context)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dan/mini_temporal/training/fine_tune_gemma_halfway.py", line 125, in other_preprocessing
    dataset = dataset.map(lambda x: tokenizer(x[f'{context_plug}_prompt']), batched=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 602, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 567, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 3156, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 3547, in _map_single
    batch = apply_function_on_filtered_inputs(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 3416, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dan/mini_temporal/training/fine_tune_gemma_halfway.py", line 125, in <lambda>
    dataset = dataset.map(lambda x: tokenizer(x[f'{context_plug}_prompt']), batched=True)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2883, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2941, in _call_one
    raise ValueError(
ValueError: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).