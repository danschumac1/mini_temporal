LOG INTO HUGGING FACE
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /home/dan/.cache/huggingface/token
Login successful
load model
tokenizer
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.86it/s]
load data
Data loaded
<bos><start_of_turn>user
Who did not ordain women until 1994?<end_of_turn>
<start_of_turn>model
The answer is: The Church of England.<end_of_turn>
<eos>
<class 'str'>



Map: 100%|██████████| 60000/60000 [00:06<00:00, 9057.43 examples/s]
<bos><start_of_turn>user
How many northern counties remained part of Britain until 1922?<end_of_turn>
<start_of_turn>model
The answer is: six.<end_of_turn>
<eos>
<class 'str'>
Data preprocessed
lora
Map: 100%|██████████| 7500/7500 [00:00<00:00, 10091.72 examples/s]
Traceback (most recent call last):
  File "/home/dan/mini_temporal/training/fine_tune_gemma_it.py", line 291, in <module>
    main()
  File "/home/dan/mini_temporal/training/fine_tune_gemma_it.py", line 228, in main
    model = get_peft_model(model, lora_config).to(device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1152, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  [Previous line repeated 5 more times]
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 825, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/dan/miniconda3/envs/danEnv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1150, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 47.53 GiB of which 110.00 MiB is free. Including non-PyTorch memory, this process has 5.31 GiB memory in use. Process 517488 has 42.03 GiB memory in use. Of the allocated memory 5.05 GiB is allocated by PyTorch, and 9.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)